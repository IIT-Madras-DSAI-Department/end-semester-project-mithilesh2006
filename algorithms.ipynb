{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "63f43f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "0cb84a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=7, min_samples_split=10,\n",
    "                 feature_subsample=0.5, n_thresholds=10, num_classes=10):\n",
    "\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.feature_subsample = feature_subsample\n",
    "        self.n_thresholds = n_thresholds\n",
    "        self.num_classes = num_classes\n",
    "        self.root = None\n",
    "\n",
    "    def _gini(self, y):\n",
    "        m = len(y)\n",
    "        if m == 0:\n",
    "            return 0.0\n",
    "        g = 1.0\n",
    "        for c in range(self.num_classes):\n",
    "            p = np.sum(y == c) / m\n",
    "            g -= p * p\n",
    "        return g\n",
    "\n",
    "    def _majority(self, y):\n",
    "        labs, cts = np.unique(y, return_counts=True)\n",
    "        return labs[np.argmax(cts)]\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        m, n = X.shape\n",
    "        n_feats = max(1, int(n * self.feature_subsample))\n",
    "        feat_idx = np.random.choice(n, n_feats, replace=False)\n",
    "\n",
    "        best_gini = 999\n",
    "        best_feat = None\n",
    "        best_thr = None\n",
    "\n",
    "        for f in feat_idx:\n",
    "            col = X[:, f]\n",
    "            thresholds = np.quantile(col, np.linspace(0.05, 0.95, self.n_thresholds))\n",
    "\n",
    "            for t in np.unique(thresholds):\n",
    "                left = col <= t\n",
    "                right = ~left\n",
    "                if left.sum() == 0 or right.sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                g_left = self._gini(y[left])\n",
    "                g_right = self._gini(y[right])\n",
    "                g_tot = (left.sum() / m) * g_left + (right.sum() / m) * g_right\n",
    "\n",
    "                if g_tot < best_gini:\n",
    "                    best_gini = g_tot\n",
    "                    best_feat = f\n",
    "                    best_thr = t\n",
    "\n",
    "        return best_feat, best_thr\n",
    "\n",
    "    def _build(self, X, y, depth):\n",
    "        if depth >= self.max_depth or len(y) < self.min_samples_split or len(np.unique(y)) == 1:\n",
    "            return {\"leaf\": True, \"value\": self._majority(y)}\n",
    "\n",
    "        f, thr = self._best_split(X, y)\n",
    "        if f is None:\n",
    "            return {\"leaf\": True, \"value\": self._majority(y)}\n",
    "\n",
    "        left = X[:, f] <= thr\n",
    "        right = ~left\n",
    "\n",
    "        return {\n",
    "            \"leaf\": False,\n",
    "            \"feat\": f,\n",
    "            \"thr\": thr,\n",
    "            \"left\": self._build(X[left], y[left], depth + 1),\n",
    "            \"right\": self._build(X[right], y[right], depth + 1)\n",
    "        }\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build(X, y, 0)\n",
    "\n",
    "    def _predict_one(self, x, node):\n",
    "        if node[\"leaf\"]:\n",
    "            return node[\"value\"]\n",
    "        if x[node[\"feat\"]] <= node[\"thr\"]:\n",
    "            return self._predict_one(x, node[\"left\"])\n",
    "        else:\n",
    "            return self._predict_one(x, node[\"right\"])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_one(x, self.root) for x in X])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "c5d3caae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, name=\"Model\"):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"TRAINING: {name}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    t1 = time.time()\n",
    "\n",
    "    print(f\"Training Time: {t1 - t0:.2f} seconds\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "7a9f32ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, X_val, y_val, name=\"Model\"):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"VALIDATION: {name}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    t0 = time.time()\n",
    "    y_pred = model.predict(X_val)\n",
    "    t1 = time.time()\n",
    "\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "    print(f\"Prediction Time: {t1 - t0:.2f} seconds\")\n",
    "\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "a423118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_precision_recall_f1(y_true, y_pred, name=\"Model\"):\n",
    "    (macro_p, macro_r, macro_f1,\n",
    "     micro_p, micro_r, micro_f1) = compute_metrics(y_true, y_pred)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"PRECISION / RECALL / F1 : {name}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Macro Precision : {macro_p:.4f}\")\n",
    "    print(f\"Macro Recall    : {macro_r:.4f}\")\n",
    "    print(f\"Macro F1        : {macro_f1:.4f}\")\n",
    "    print(f\"Micro Precision : {micro_p:.4f}\")\n",
    "    print(f\"Micro Recall    : {micro_r:.4f}\")\n",
    "    print(f\"Micro F1        : {micro_f1:.4f}\")\n",
    "    print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fef64b",
   "metadata": {},
   "source": [
    "RANDAM FOREST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc51ff61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "28c5711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier:\n",
    "    def __init__(self, n_estimators=20, subsample=0.7, feature_subsample=0.3,\n",
    "                 n_thresholds=10, max_depth=8, min_samples_split=10, num_classes=10):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.subsample = subsample\n",
    "        self.feature_subsample = feature_subsample\n",
    "        self.n_thresholds = n_thresholds\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.num_classes = num_classes\n",
    "        self.trees = []\n",
    "        self.training_time_ = 0.0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        start = time.time()\n",
    "        m = len(y)\n",
    "        self.trees = []\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            n_samples = int(m * self.subsample)\n",
    "            idx = np.random.choice(m, n_samples, replace=True)\n",
    "            X_bag = X[idx]\n",
    "            y_bag = y[idx]\n",
    "\n",
    "            tree = DecisionTree(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                feature_subsample=self.feature_subsample,\n",
    "                n_thresholds=self.n_thresholds,\n",
    "                num_classes=self.num_classes\n",
    "            )\n",
    "            tree.fit(X_bag, y_bag)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "            if (i+1) % 5 == 0 or i == 0:\n",
    "                print(f\"[RandomForest] Trained {i+1}/{self.n_estimators} trees\")\n",
    "\n",
    "        end = time.time()\n",
    "        self.training_time_ = end - start\n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        if len(self.trees) == 0:\n",
    "            return np.zeros(len(X), dtype=int)\n",
    "\n",
    "        \n",
    "        all_preds = np.zeros((len(X), self.n_estimators), dtype=int)\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            all_preds[:, i] = tree.predict(X)\n",
    "\n",
    "        \n",
    "        y_pred = np.zeros(len(X), dtype=int)\n",
    "        for i in range(len(X)):\n",
    "            labels, counts = np.unique(all_preds[i], return_counts=True)\n",
    "            y_pred[i] = labels[np.argmax(counts)]\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "928571a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate_random_forest(X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    print(\"   TRAINING RANDOM FOREST\")\n",
    "    \n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=20,\n",
    "        subsample=0.7,\n",
    "        feature_subsample=0.3,\n",
    "        n_thresholds=10,\n",
    "        max_depth=8,\n",
    "        min_samples_split=10,\n",
    "        num_classes=10\n",
    "    )\n",
    "\n",
    "    # ----- TRAIN -----\n",
    "    t0 = time.time()\n",
    "    rf.fit(X_train, y_train)\n",
    "    train_time = time.time() - t0\n",
    "\n",
    "    # ----- VALIDATE -----\n",
    "    t0 = time.time()\n",
    "    y_pred = rf.predict(X_val)\n",
    "    pred_time = time.time() - t0\n",
    "\n",
    "    # ----- METRICS -----\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    mp, mr, mf1, microp, micror, microf1 = compute_metrics(y_val, y_pred)\n",
    "\n",
    "    \n",
    "    print(\"     RANDOM FOREST RESULTS\")\n",
    "    \n",
    "    print(f\"Accuracy         : {acc:.4f}\")\n",
    "    print(f\"Macro Precision  : {mp:.4f}\")\n",
    "    print(f\"Macro Recall     : {mr:.4f}\")\n",
    "    print(f\"Macro F1 Score   : {mf1:.4f}\")\n",
    "    print(f\"Micro Precision  : {microp:.4f}\")\n",
    "    print(f\"Micro Recall     : {micror:.4f}\")\n",
    "    print(f\"Micro F1 Score   : {microf1:.4f}\")\n",
    "    print(f\"Training Time    : {train_time:.2f} sec\")\n",
    "    print(f\"Prediction Time  : {pred_time:.2f} sec\")\n",
    "    print(\"==============================\\n\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"macro_p\": mp,\n",
    "        \"macro_r\": mr,\n",
    "        \"macro_f1\": mf1,\n",
    "        \"micro_p\": microp,\n",
    "        \"micro_r\": micror,\n",
    "        \"micro_f1\": microf1,\n",
    "        \"train_time\": train_time,\n",
    "        \"pred_time\": pred_time\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "f77fc4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TRAINING RANDOM FOREST\n",
      "[RandomForest] Trained 1/20 trees\n",
      "[RandomForest] Trained 5/20 trees\n",
      "[RandomForest] Trained 10/20 trees\n",
      "[RandomForest] Trained 15/20 trees\n",
      "[RandomForest] Trained 20/20 trees\n",
      "     RANDOM FOREST RESULTS\n",
      "Accuracy         : 0.9352\n",
      "Macro Precision  : 0.1871\n",
      "Macro Recall     : 0.1870\n",
      "Macro F1 Score   : 0.1870\n",
      "Micro Precision  : 0.9352\n",
      "Micro Recall     : 0.9352\n",
      "Micro F1 Score   : 0.9352\n",
      "Training Time    : 213.44 sec\n",
      "Prediction Time  : 0.16 sec\n",
      "==============================\n",
      "\n",
      "Random Forest Results: {'accuracy': 0.9351740696278511, 'macro_p': 0.18707371717596594, 'macro_r': 0.1870034239115112, 'macro_f1': 0.18702656868277154, 'micro_p': 0.9351740696278508, 'micro_r': 0.9351740696278508, 'micro_f1': 0.9351740696273507, 'train_time': 213.4402813911438, 'pred_time': 0.1639707088470459}\n"
     ]
    }
   ],
   "source": [
    "X_train_raw, y_train, X_val_raw, y_val = load_mnist()\n",
    "rf_results = train_and_validate_random_forest(X_train_raw, y_train, X_val_raw, y_val)\n",
    "print(\"Random Forest Results:\", rf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "f42191d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def softmax(z):\n",
    "    z = z - np.max(z, axis=1, keepdims=True)\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def add_bias(X):\n",
    "    m = X.shape[0]\n",
    "    return np.hstack([np.ones((m,1)), X])\n",
    "\n",
    "def accuracy_score(y_true,y_pred):\n",
    "    return np.mean(y_true==y_pred)\n",
    "\n",
    "def compute_metrics(y_true,y_pred,num_classes=10):\n",
    "    eps = 1e-12\n",
    "    precisions=[]; recalls=[]; f1s=[]\n",
    "    tp_global=fp_global=fn_global=0\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        tp=np.sum((y_pred==c)&(y_true==c))\n",
    "        fp=np.sum((y_pred==c)&(y_true!=c))\n",
    "        fn=np.sum((y_pred!=c)&(y_true==c))\n",
    "\n",
    "        tp_global+=tp; fp_global+=fp; fn_global+=fn\n",
    "\n",
    "        p=tp/(tp+fp+eps)\n",
    "        r=tp/(tp+fn+eps)\n",
    "        f1=2*p*r/(p+r+eps)\n",
    "\n",
    "        precisions.append(p)\n",
    "        recalls.append(r)\n",
    "        f1s.append(f1)\n",
    "\n",
    "    macro_p=np.mean(precisions)\n",
    "    macro_r=np.mean(recalls)\n",
    "    macro_f1=np.mean(f1s)\n",
    "\n",
    "    micro_p=tp_global/(tp_global+fp_global+eps)\n",
    "    micro_r=tp_global/(tp_global+fn_global+eps)\n",
    "    micro_f1=2*micro_p*micro_r/(micro_p+micro_r+eps)\n",
    "\n",
    "    return macro_p,macro_r,macro_f1,micro_p,micro_r,micro_f1\n",
    "\n",
    "\n",
    "class PCA:\n",
    "    def __init__(self,n_components=60,max_samples_for_fit=20000):\n",
    "        self.n_components=n_components\n",
    "        self.max_samples_for_fit=max_samples_for_fit\n",
    "\n",
    "    def fit(self,X):\n",
    "        m=X.shape[0]\n",
    "        if m>self.max_samples_for_fit:\n",
    "            idx=np.random.choice(m,self.max_samples_for_fit,replace=False)\n",
    "            X=X[idx]\n",
    "\n",
    "        self.mean_=np.mean(X,axis=0)\n",
    "        Xc=X-self.mean_\n",
    "        U,S,Vt=np.linalg.svd(Xc,full_matrices=False)\n",
    "        self.components_=Vt[:self.n_components].T\n",
    "\n",
    "    def transform(self,X):\n",
    "        return (X-self.mean_) @ self.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "1e3356dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(train_path=\"MNIST_train.csv\",val_path=\"MNIST_validation.csv\"):\n",
    "    train=pd.read_csv(train_path)\n",
    "    val=pd.read_csv(val_path)\n",
    "\n",
    "    X_train=train.iloc[:,1:-1].values/255.0\n",
    "    y_train=train.iloc[:,-1].values.astype(int)\n",
    "\n",
    "    X_val=val.iloc[:,1:-1].values/255.0\n",
    "    y_val=val.iloc[:,-1].values.astype(int)\n",
    "\n",
    "    return X_train,y_train,X_val,y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "354aa921",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LOGISTIC REGRESSION OVR\n",
    "\n",
    "def _fit_binary_logistic(X,y,lr,epochs,bs):\n",
    "    Xb=add_bias(X)\n",
    "    m,n=Xb.shape\n",
    "    y=y.reshape(-1,1)\n",
    "    w=np.zeros((n,1))\n",
    "\n",
    "    for e in range(epochs):\n",
    "        idx=np.random.permutation(m)\n",
    "        Xb_sh=Xb[idx]; y_sh=y[idx]\n",
    "        for i in range(0,m,bs):\n",
    "            Xb_i=Xb_sh[i:i+bs]\n",
    "            y_i=y_sh[i:i+bs]\n",
    "            pred=sigmoid(Xb_i@w)\n",
    "            grad=Xb_i.T@(pred-y_i)/Xb_i.shape[0]\n",
    "            w-=lr*grad\n",
    "\n",
    "    return w\n",
    "\n",
    "class OvRLogisticRegression:\n",
    "    def __init__(self,lr=0.05,epochs=8,bs=128,num_classes=10):\n",
    "        self.lr=lr; self.epochs=epochs; self.bs=bs; self.C=num_classes\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        Xb=add_bias(X)\n",
    "        m,n=Xb.shape\n",
    "        self.W=np.zeros((self.C,n))\n",
    "        for c in range(self.C):\n",
    "            print(f\"[OvR] Training class {c}\")\n",
    "            w=_fit_binary_logistic(X,(y==c).astype(int),\n",
    "                                   self.lr,self.epochs,self.bs)\n",
    "            self.W[c]=w.ravel()\n",
    "\n",
    "    def predict(self,X):\n",
    "        Xb=add_bias(X)\n",
    "        logits=Xb@self.W.T\n",
    "        return np.argmax(sigmoid(logits),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251afd03",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "491a2e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING: Logistic OvR\n",
      "============================================================\n",
      "[OvR] Training class 0\n",
      "[OvR] Training class 1\n",
      "[OvR] Training class 2\n",
      "[OvR] Training class 3\n",
      "[OvR] Training class 4\n",
      "[OvR] Training class 5\n",
      "[OvR] Training class 6\n",
      "[OvR] Training class 7\n",
      "[OvR] Training class 8\n",
      "[OvR] Training class 9\n",
      "Training Time: 0.41 seconds\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "VALIDATION: Logistic OvR\n",
      "============================================================\n",
      "Validation Accuracy: 0.8583\n",
      "Prediction Time: 0.00 seconds\n",
      "\n",
      "============================================================\n",
      "PRECISION / RECALL / F1 : Logistic OvR\n",
      "============================================================\n",
      "Macro Precision : 0.1717\n",
      "Macro Recall    : 0.1716\n",
      "Macro F1        : 0.1716\n",
      "Micro Precision : 0.8583\n",
      "Micro Recall    : 0.8583\n",
      "Micro F1        : 0.8583\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "logi = OvRLogisticRegression()\n",
    "train_model(logi, X_train, y_train, \"Logistic OvR\")\n",
    "\n",
    "logi_pred = validate_model(logi, X_val, y_val, \"Logistic OvR\")\n",
    "print_precision_recall_f1(y_val, logi_pred, \"Logistic OvR\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "cc3e8117",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MODEL EVALUATION\n",
    "\n",
    "def evaluate_model(name,model,X_train,y_train,X_val,y_val,train=True):\n",
    "    print(\"\\n\"+\"=\"*80)\n",
    "    print(\"MODEL:\",name)\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    if train:\n",
    "        t0=time.time()\n",
    "        model.fit(X_train,y_train)\n",
    "        train_time=time.time()-t0\n",
    "    else:\n",
    "        train_time=0.0\n",
    "\n",
    "    t0=time.time()\n",
    "    y_pred=model.predict(X_val)\n",
    "    pred_time=time.time()-t0\n",
    "\n",
    "    acc=accuracy_score(y_val,y_pred)\n",
    "    (mp, mr, mf1, micp, micr, micf1)=compute_metrics(y_val,y_pred)\n",
    "\n",
    "    print(f\"Accuracy        : {acc:.4f}\")\n",
    "    print(f\"Macro Precision : {mp:.4f}\")\n",
    "    print(f\"Macro Recall    : {mr:.4f}\")\n",
    "    print(f\"Macro F1        : {mf1:.4f}\")\n",
    "    print(f\"Micro Precision : {micp:.4f}\")\n",
    "    print(f\"Micro Recall    : {micr:.4f}\")\n",
    "    print(f\"Micro F1        : {micf1:.4f}\")\n",
    "    print(f\"Training Time   : {train_time:.2f}\")\n",
    "    print(f\"Prediction Time : {pred_time:.2f}\")\n",
    "\n",
    "    return {\n",
    "        \"name\":name,\n",
    "        \"accuracy\":acc,\n",
    "        \"macro_p\":mp,\n",
    "        \"macro_r\":mr,\n",
    "        \"macro_f1\":mf1,\n",
    "        \"micro_p\":micp,\n",
    "        \"micro_r\":micr,\n",
    "        \"micro_f1\":micf1,\n",
    "        \"train_time\":train_time,\n",
    "        \"pred_time\":pred_time\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# SOFTMAX REGRESSION\n",
    "\n",
    "class SoftmaxRegression:\n",
    "    def __init__(self,lr=0.1,epochs=25,num_classes=10):\n",
    "        self.lr=lr\n",
    "        self.epochs=epochs\n",
    "        self.C=num_classes\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        m,n=X.shape\n",
    "        self.W=np.zeros((n,self.C))\n",
    "        self.b=np.zeros((1,self.C))\n",
    "\n",
    "        Y=np.zeros((m,self.C))\n",
    "        Y[np.arange(m),y]=1\n",
    "\n",
    "        for e in range(1,self.epochs+1):\n",
    "            logits=X@self.W+self.b\n",
    "            probs=softmax(logits)\n",
    "\n",
    "            grad=(probs-Y)/m\n",
    "            self.W-=self.lr*(X.T@grad)\n",
    "            self.b-=self.lr*np.sum(grad,axis=0,keepdims=True)\n",
    "\n",
    "            if e%5==0:\n",
    "                loss=-np.mean(np.sum(Y*np.log(probs+1e-12),axis=1))\n",
    "                print(f\"[Softmax] Epoch {e}/{self.epochs}, Loss={loss:.4f}\")\n",
    "\n",
    "    def predict(self,X):\n",
    "        return np.argmax(softmax(X@self.W+self.b),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "e5040f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING: Softmax Regression\n",
      "============================================================\n",
      "[Softmax] Epoch 5/25, Loss=1.8538\n",
      "[Softmax] Epoch 10/25, Loss=1.4694\n",
      "[Softmax] Epoch 15/25, Loss=1.2289\n",
      "[Softmax] Epoch 20/25, Loss=1.0721\n",
      "[Softmax] Epoch 25/25, Loss=0.9628\n",
      "Training Time: 0.11 seconds\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "VALIDATION: Softmax Regression\n",
      "============================================================\n",
      "Validation Accuracy: 0.8219\n",
      "Prediction Time: 0.00 seconds\n",
      "\n",
      "============================================================\n",
      "PRECISION / RECALL / F1 : Softmax Regression\n",
      "============================================================\n",
      "Macro Precision : 0.1650\n",
      "Macro Recall    : 0.1642\n",
      "Macro F1        : 0.1642\n",
      "Micro Precision : 0.8219\n",
      "Micro Recall    : 0.8219\n",
      "Micro F1        : 0.8219\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "soft = SoftmaxRegression()\n",
    "train_model(soft, X_train, y_train, \"Softmax Regression\")\n",
    "soft_pred = validate_model(soft, X_val, y_val, \"Softmax Regression\")\n",
    "\n",
    "print_precision_recall_f1(y_val, soft_pred, \"Softmax Regression\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "eecdfa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class XGBStump:\n",
    "    def __init__(self, lambda_reg=1.0):\n",
    "        self.f_idx = None\n",
    "        self.thresh = None\n",
    "        self.left_val = None\n",
    "        self.right_val = None\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def fit(self, X, g, h, n_thresholds=20, feature_subsample=0.4):\n",
    "        m, n = X.shape\n",
    "        n_feats = max(1, int(n * feature_subsample))\n",
    "        feat_idx_list = np.random.choice(n, n_feats, replace=False)\n",
    "\n",
    "        best_gain = -1e18\n",
    "        G_total = np.sum(g)\n",
    "        H_total = np.sum(h)\n",
    "\n",
    "        parent_gain = (G_total**2) / (H_total + self.lambda_reg)\n",
    "\n",
    "        for f in feat_idx_list:\n",
    "            Xf = X[:, f]\n",
    "            thresholds = np.quantile(Xf, np.linspace(0.05, 0.95, n_thresholds))\n",
    "\n",
    "            for t in np.unique(thresholds):\n",
    "                left = (Xf <= t)\n",
    "                if left.sum() == 0 or left.sum() == m:\n",
    "                    continue\n",
    "\n",
    "                G_left = g[left].sum()\n",
    "                H_left = h[left].sum()\n",
    "                G_right = G_total - G_left\n",
    "                H_right = H_total - H_left\n",
    "\n",
    "                gain = (G_left**2)/(H_left+self.lambda_reg) + \\\n",
    "                       (G_right**2)/(H_right+self.lambda_reg) - parent_gain\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    self.f_idx = f\n",
    "                    self.thresh = t\n",
    "                    self.left_val  = - G_left  / (H_left  + self.lambda_reg)\n",
    "                    self.right_val = - G_right / (H_right + self.lambda_reg)\n",
    "\n",
    "    def predict(self, X):\n",
    "        col = X[:, self.f_idx]\n",
    "        return np.where(col <= self.thresh, self.left_val, self.right_val)\n",
    "\n",
    "class XGBoostBinary:\n",
    "    def __init__(self, n_estimators=120, lr=0.15, lambda_reg=1.0):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.lr = lr\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.stumps = []\n",
    "        self.base_score = 0.0\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -30, 30)))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        eps = 1e-9\n",
    "        \n",
    "        p0 = np.clip(np.mean(y), eps, 1 - eps)\n",
    "        self.base_score = np.log(p0/(1-p0))\n",
    "\n",
    "        y_pred = np.full(len(y), self.base_score)\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            p = self.sigmoid(y_pred)\n",
    "            g = p - y\n",
    "            h = p * (1 - p)\n",
    "\n",
    "            stump = XGBStump(lambda_reg=self.lambda_reg)\n",
    "            stump.fit(X, g, h)\n",
    "\n",
    "            update = stump.predict(X)\n",
    "            y_pred += self.lr * update\n",
    "\n",
    "            self.stumps.append(stump)\n",
    "\n",
    "            if (i+1) % 20 == 0:\n",
    "                print(f\"[XGB-Binary] Tree {i+1}/{self.n_estimators}\")\n",
    "                \n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        pred = np.full(X.shape[0], self.base_score)\n",
    "        for stump in self.stumps:\n",
    "            pred += self.lr * stump.predict(X)\n",
    "        return self.sigmoid(pred)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "class XGBoostMulti:\n",
    "    def __init__(self, num_classes=10, n_estimators=120, lr=0.15):\n",
    "        self.C = num_classes\n",
    "        self.models = [\n",
    "            XGBoostBinary(n_estimators=n_estimators, lr=lr)\n",
    "            for _ in range(self.C)\n",
    "        ]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        print(\"\\n TRAINING XGBOOST (MULTI-CLASS) \")\n",
    "        for c in range(self.C):\n",
    "            \n",
    "            y_binary = (y == c).astype(int)\n",
    "            self.models[c].fit(X, y_binary)\n",
    "\n",
    "    def predict(self, X):\n",
    "        scores = np.zeros((X.shape[0], self.C))\n",
    "        for c in range(self.C):\n",
    "            scores[:, c] = self.models[c].predict_proba(X)\n",
    "        return np.argmax(scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "f3e2dad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING: XGBoost Multi\n",
      "============================================================\n",
      "\n",
      " TRAINING XGBOOST (MULTI-CLASS) \n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "Training Time: 103.05 seconds\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "VALIDATION: XGBoost Multi\n",
      "============================================================\n",
      "Validation Accuracy: 0.8535\n",
      "Prediction Time: 0.01 seconds\n",
      "\n",
      "============================================================\n",
      "PRECISION / RECALL / F1 : XGBoost Multi\n",
      "============================================================\n",
      "Macro Precision : 0.1707\n",
      "Macro Recall    : 0.1707\n",
      "Macro F1        : 0.1707\n",
      "Micro Precision : 0.8535\n",
      "Micro Recall    : 0.8535\n",
      "Micro F1        : 0.8535\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBoostMulti(num_classes=10, n_estimators=120, lr=0.15)\n",
    "train_model(xgb, X_train, y_train, \"XGBoost Multi\")\n",
    "xgb_pred = validate_model(xgb, X_val, y_val, \"XGBoost Multi\")\n",
    "print_precision_recall_f1(y_val, xgb_pred, \"XGBoost Multi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "279d0291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, y_pred, num_classes=10):\n",
    "    eps = 1e-12\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "\n",
    "    tp_global = fp_global = fn_global = 0\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        tp = np.sum((y_pred == c) & (y_true == c))\n",
    "        fp = np.sum((y_pred == c) & (y_true != c))\n",
    "        fn = np.sum((y_pred != c) & (y_true == c))\n",
    "\n",
    "        tp_global += tp\n",
    "        fp_global += fp\n",
    "        fn_global += fn\n",
    "\n",
    "        precision = tp / (tp + fp + eps)\n",
    "        recall = tp / (tp + fn + eps)\n",
    "        f1 = 2 * precision * recall / (precision + recall + eps)\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "\n",
    "    macro_p = np.mean(precisions)\n",
    "    macro_r = np.mean(recalls)\n",
    "    macro_f1 = np.mean(f1s)\n",
    "\n",
    "    micro_p = tp_global / (tp_global + fp_global + eps)\n",
    "    micro_r = tp_global / (tp_global + fn_global + eps)\n",
    "    micro_f1 = 2 * micro_p * micro_r / (micro_p + micro_r + eps)\n",
    "\n",
    "    return macro_p, macro_r, macro_f1, micro_p, micro_r, micro_f1\n",
    "\n",
    "\n",
    "\n",
    "class PCA:\n",
    "    def __init__(self, n_components=100, max_samples_for_fit=20000):\n",
    "        self.n_components = n_components\n",
    "        self.max_samples_for_fit = max_samples_for_fit\n",
    "        self.mean_ = None\n",
    "        self.components_ = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        m = X.shape[0]\n",
    "\n",
    "        if m > self.max_samples_for_fit:\n",
    "            idx = np.random.choice(m, self.max_samples_for_fit, replace=False)\n",
    "            X = X[idx]\n",
    "\n",
    "        self.mean_ = np.mean(X, axis=0)\n",
    "        X_centered = X - self.mean_\n",
    "\n",
    "        U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
    "        self.components_ = Vt[:self.n_components].T  \n",
    "\n",
    "    def transform(self, X):\n",
    "        return (X - self.mean_) @ self.components_\n",
    "\n",
    "\n",
    "\n",
    "class KNNClassifier:\n",
    "    def __init__(self, k=5, max_train_samples=60000):\n",
    "        self.k = k\n",
    "        self.max_train_samples = max_train_samples\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        m = X.shape[0]\n",
    "\n",
    "        if m > self.max_train_samples:\n",
    "            idx = np.random.choice(m, self.max_train_samples, replace=False)\n",
    "            self.X = X[idx].astype(np.float32)\n",
    "            self.y = y[idx].astype(int)\n",
    "            print(f\"[KNN] Using {self.X.shape[0]} subsampled training samples.\")\n",
    "        else:\n",
    "            self.X = X.astype(np.float32)\n",
    "            self.y = y.astype(int)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = X.astype(np.float32)\n",
    "        m_test = X.shape[0]\n",
    "        y_pred = np.zeros(m_test, int)\n",
    "\n",
    "        batch = 200\n",
    "        for i in range(0, m_test, batch):\n",
    "            Xb = X[i:i + batch]\n",
    "\n",
    "            dists = (\n",
    "                np.sum(Xb**2, axis=1, keepdims=True)\n",
    "                + np.sum(self.X**2, axis=1, keepdims=True).T\n",
    "                - 2 * (Xb @ self.X.T)\n",
    "        )\n",
    "\n",
    "\n",
    "            knn_idx = np.argpartition(dists, self.k, axis=1)[:, :self.k]\n",
    "            labels = self.y[knn_idx]\n",
    "\n",
    "            for j in range(labels.shape[0]):\n",
    "                labs, counts = np.unique(labels[j], return_counts=True)\n",
    "                y_pred[i+j] = labs[np.argmax(counts)]\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_mnist(train_path=\"MNIST_train.csv\", val_path=\"MNIST_validation.csv\"):\n",
    "    print(\"Loading MNIST...\")\n",
    "    train = pd.read_csv(train_path)\n",
    "    val = pd.read_csv(val_path)\n",
    "\n",
    "    train.drop(columns=['even'], inplace=True)\n",
    "    val.drop(columns=['even'], inplace=True)\n",
    "\n",
    "    X_train = train.iloc[:, 1:].values.astype(np.float32) / 255.0\n",
    "    y_train = train.iloc[:, 0].values.astype(int)\n",
    "\n",
    "    X_val = val.iloc[:, 1:].values.astype(np.float32) / 255.0\n",
    "    y_val = val.iloc[:, 0].values.astype(int)\n",
    "\n",
    "    print(\"Train:\", X_train.shape, \"Val:\", X_val.shape)\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "e83c96e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_knn_train(knn, X_train, y_train):\n",
    "    print(\"\\n========== TRAIN SET EVALUATION ==========\")\n",
    "    \n",
    "    t0 = time.time()\n",
    "    y_pred_train = knn.predict(X_train)\n",
    "    pred_time_train = time.time() - t0\n",
    "\n",
    "    acc = np.mean(y_pred_train == y_train)\n",
    "    mp, mr, mf1, microp, micror, microf1 = compute_metrics(y_train, y_pred_train)\n",
    "\n",
    "    print(f\"Train Accuracy        : {acc:.4f}\")\n",
    "    print(f\"Train Macro Precision : {mp:.4f}\")\n",
    "    print(f\"Train Macro Recall    : {mr:.4f}\")\n",
    "    print(f\"Train Macro F1        : {mf1:.4f}\")\n",
    "    print(f\"Train Micro Precision : {microp:.4f}\")\n",
    "    print(f\"Train Micro Recall    : {micror:.4f}\")\n",
    "    print(f\"Train Micro F1        : {microf1:.4f}\")\n",
    "    print(f\"Train Prediction Time : {pred_time_train:.2f}s\")\n",
    "\n",
    "    return y_pred_train\n",
    "\n",
    "\n",
    "def evaluate_knn_val(knn, X_val, y_val):\n",
    "    print(\"\\n========== VALIDATION SET EVALUATION ==========\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    y_pred_val = knn.predict(X_val)\n",
    "    pred_time_val = time.time() - t0\n",
    "\n",
    "    acc = np.mean(y_pred_val == y_val)\n",
    "    mp, mr, mf1, microp, micror, microf1 = compute_metrics(y_val, y_pred_val)\n",
    "\n",
    "    print(f\"Validation Accuracy        : {acc:.4f}\")\n",
    "    print(f\"Validation Macro Precision : {mp:.4f}\")\n",
    "    print(f\"Validation Macro Recall    : {mr:.4f}\")\n",
    "    print(f\"Validation Macro F1        : {mf1:.4f}\")\n",
    "    print(f\"Validation Micro Precision : {microp:.4f}\")\n",
    "    print(f\"Validation Micro Recall    : {micror:.4f}\")\n",
    "    print(f\"Validation Micro F1        : {microf1:.4f}\")\n",
    "    print(f\"Validation Prediction Time : {pred_time_val:.2f}s\")\n",
    "\n",
    "    return y_pred_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "5e31d91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST...\n",
      "Train: (10002, 784) Val: (2499, 784)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "a318f514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training KNN model...\n",
      "KNN Training Time: 0.00s\n",
      "\n",
      "========== TRAIN SET EVALUATION ==========\n",
      "Train Accuracy        : 0.9834\n",
      "Train Macro Precision : 0.1967\n",
      "Train Macro Recall    : 0.1967\n",
      "Train Macro F1        : 0.1967\n",
      "Train Micro Precision : 0.9834\n",
      "Train Micro Recall    : 0.9834\n",
      "Train Micro F1        : 0.9834\n",
      "Train Prediction Time : 2.58s\n",
      "\n",
      "========== VALIDATION SET EVALUATION ==========\n",
      "Validation Accuracy        : 0.9784\n",
      "Validation Macro Precision : 0.1957\n",
      "Validation Macro Recall    : 0.1956\n",
      "Validation Macro F1        : 0.1957\n",
      "Validation Micro Precision : 0.9784\n",
      "Validation Micro Recall    : 0.9784\n",
      "Validation Micro F1        : 0.9784\n",
      "Validation Prediction Time : 0.81s\n"
     ]
    }
   ],
   "source": [
    "knn = KNNClassifier(k=5, max_train_samples=20000)\n",
    "\n",
    "\n",
    "print(\"\\nTraining KNN model...\")\n",
    "t0 = time.time()\n",
    "knn.fit(X_train, y_train)\n",
    "train_time = time.time() - t0\n",
    "print(f\"KNN Training Time: {train_time:.2f}s\")\n",
    "\n",
    "train_preds = evaluate_knn_train(knn, X_train, y_train)\n",
    "\n",
    "\n",
    "val_preds = evaluate_knn_val(knn, X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "07642a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST...\n",
      "Train: (10002, 784) Val: (2499, 784)\n",
      "\n",
      "Fitting PCA...\n",
      "PCA completed. Features: 60\n",
      "\n",
      "================================================================================\n",
      "MODEL: Softmax\n",
      "================================================================================\n",
      "[Softmax] Epoch 5/25, Loss=1.9130\n",
      "[Softmax] Epoch 10/25, Loss=1.5746\n",
      "[Softmax] Epoch 15/25, Loss=1.3518\n",
      "[Softmax] Epoch 20/25, Loss=1.1987\n",
      "[Softmax] Epoch 25/25, Loss=1.0882\n",
      "Accuracy        : 0.7807\n",
      "Macro Precision : 0.8067\n",
      "Macro Recall    : 0.7743\n",
      "Macro F1        : 0.7734\n",
      "Micro Precision : 0.7807\n",
      "Micro Recall    : 0.7807\n",
      "Micro F1        : 0.7807\n",
      "Training Time   : 0.22\n",
      "Prediction Time : 0.00\n",
      "\n",
      "================================================================================\n",
      "MODEL: Logistic OvR\n",
      "================================================================================\n",
      "[OvR] Training class 0\n",
      "[OvR] Training class 1\n",
      "[OvR] Training class 2\n",
      "[OvR] Training class 3\n",
      "[OvR] Training class 4\n",
      "[OvR] Training class 5\n",
      "[OvR] Training class 6\n",
      "[OvR] Training class 7\n",
      "[OvR] Training class 8\n",
      "[OvR] Training class 9\n",
      "Accuracy        : 0.8647\n",
      "Macro Precision : 0.8661\n",
      "Macro Recall    : 0.8622\n",
      "Macro F1        : 0.8625\n",
      "Micro Precision : 0.8647\n",
      "Micro Recall    : 0.8647\n",
      "Micro F1        : 0.8647\n",
      "Training Time   : 0.53\n",
      "Prediction Time : 0.00\n",
      "\n",
      "================================================================================\n",
      "MODEL: Random Forest\n",
      "================================================================================\n",
      "[RandomForest] Trained 1/10 trees\n",
      "[RandomForest] Trained 5/10 trees\n",
      "[RandomForest] Trained 10/10 trees\n",
      "Accuracy        : 0.7223\n",
      "Macro Precision : 0.7424\n",
      "Macro Recall    : 0.7149\n",
      "Macro F1        : 0.7056\n",
      "Micro Precision : 0.7223\n",
      "Micro Recall    : 0.7223\n",
      "Micro F1        : 0.7223\n",
      "Training Time   : 13.53\n",
      "Prediction Time : 0.10\n",
      "\n",
      "================================================================================\n",
      "MODEL: XGBoost (NumPy)\n",
      "================================================================================\n",
      "\n",
      " TRAINING XGBOOST (MULTI-CLASS) \n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "Accuracy        : 0.8467\n",
      "Macro Precision : 0.8469\n",
      "Macro Recall    : 0.8439\n",
      "Macro F1        : 0.8440\n",
      "Micro Precision : 0.8467\n",
      "Micro Recall    : 0.8467\n",
      "Micro F1        : 0.8467\n",
      "Training Time   : 95.66\n",
      "Prediction Time : 0.02\n",
      "\n",
      "\n",
      "SUMMARY\n",
      "------------------------------------------------------------\n",
      "{'name': 'Softmax', 'accuracy': 0.7807122849139656, 'macro_p': 0.8066939487991076, 'macro_r': 0.7742845267590429, 'macro_f1': 0.7734368795310023, 'micro_p': 0.7807122849139653, 'micro_r': 0.7807122849139653, 'micro_f1': 0.7807122849134652, 'train_time': 0.22304749488830566, 'pred_time': 0.0}\n",
      "{'name': 'Logistic OvR', 'accuracy': 0.8647458983593438, 'macro_p': 0.8661342972317119, 'macro_r': 0.8621944754414255, 'macro_f1': 0.8624717903673457, 'micro_p': 0.8647458983593435, 'micro_r': 0.8647458983593435, 'micro_f1': 0.8647458983588434, 'train_time': 0.5332510471343994, 'pred_time': 0.0}\n",
      "{'name': 'Random Forest', 'accuracy': 0.7222889155662265, 'macro_p': 0.7423799551533004, 'macro_r': 0.7148986359895806, 'macro_f1': 0.7055537967213619, 'micro_p': 0.7222889155662262, 'micro_r': 0.7222889155662262, 'micro_f1': 0.7222889155657262, 'train_time': 13.527028322219849, 'pred_time': 0.10335803031921387}\n",
      "{'name': 'XGBoost (NumPy)', 'accuracy': 0.8467386954781913, 'macro_p': 0.846895254539976, 'macro_r': 0.8438958239852072, 'macro_f1': 0.8440328534144761, 'micro_p': 0.8467386954781909, 'micro_r': 0.8467386954781909, 'micro_f1': 0.8467386954776909, 'train_time': 95.65895533561707, 'pred_time': 0.015719890594482422}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# MAIN\n",
    "if __name__==\"__main__\":\n",
    "    np.random.seed(42)\n",
    "\n",
    "    X_train_raw,y_train,X_val_raw,y_val=load_mnist()\n",
    "\n",
    "    print(\"\\nFitting PCA...\")\n",
    "    pca=PCA(n_components=60)\n",
    "    pca.fit(X_train_raw)\n",
    "    X_train=pca.transform(X_train_raw)\n",
    "    X_val=pca.transform(X_val_raw)\n",
    "    print(\"PCA completed. Features:\",X_train.shape[1])\n",
    "\n",
    "    results=[]\n",
    "\n",
    "    \n",
    "    soft=SoftmaxRegression()\n",
    "    results.append(evaluate_model(\"Softmax\",soft,X_train,y_train,X_val,y_val))\n",
    "\n",
    "    \n",
    "    logi=OvRLogisticRegression()\n",
    "    results.append(evaluate_model(\"Logistic OvR\",logi,X_train,y_train,X_val,y_val))\n",
    "   \n",
    "   \n",
    "    results.append(evaluate_model(\"Random Forest\", rf_clf, X_train, y_train, X_val, y_val))\n",
    "    # kNN\n",
    "    \n",
    "    \n",
    "    xgb = XGBoostMulti(num_classes=10, n_estimators=120, lr=0.15)\n",
    "    results.append(evaluate_model(\"XGBoost (NumPy)\", xgb, X_train, y_train, X_val, y_val))\n",
    "\n",
    "    print(\"\\n\\nSUMMARY\\n\"+\"-\"*60)\n",
    "    for r in results:\n",
    "        print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22f5e84",
   "metadata": {},
   "source": [
    "STACKING -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9ef063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f105c176",
   "metadata": {},
   "source": [
    "stack_knn_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "a44e3781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Stacked_KNN_Softmax:\n",
    "    \n",
    "\n",
    "    def __init__(self, knn_model, softmax_model, meta_model):\n",
    "        self.knn = knn_model\n",
    "        self.soft = softmax_model\n",
    "        self.meta = meta_model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        print(\"\\n[Stack] Training KNN...\")\n",
    "        self.knn.fit(X, y)\n",
    "\n",
    "        print(\"[Stack] Training Softmax...\")\n",
    "        self.soft.fit(X, y)\n",
    "\n",
    "        print(\"[Stack] Training Meta-Model (OvR Logistic)...\")\n",
    "        knn_pred = self.knn.predict(X)\n",
    "        soft_pred = self.soft.predict(X)\n",
    "\n",
    "        \n",
    "        stacked_X = np.column_stack((knn_pred, soft_pred))\n",
    "\n",
    "        self.meta.fit(stacked_X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        knn_pred = self.knn.predict(X)\n",
    "        soft_pred = self.soft.predict(X)\n",
    "\n",
    "        stacked_X = np.column_stack((knn_pred, soft_pred))\n",
    "\n",
    "        return self.meta.predict(stacked_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacb1c58",
   "metadata": {},
   "source": [
    "stack_knn_rf_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "a5026365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Stacked_KNN_RF_Logistic:\n",
    "    \n",
    "    def __init__(self, knn_model, rf_model, logistic_model, meta_softmax):\n",
    "        self.knn = knn_model\n",
    "        self.rf = rf_model\n",
    "        self.logi = logistic_model\n",
    "        self.meta = meta_softmax\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        print(\"\\n[Stack] Training KNN...\")\n",
    "        self.knn.fit(X, y)\n",
    "\n",
    "        print(\"[Stack] Training Random Forest...\")\n",
    "        self.rf.fit(X, y)\n",
    "\n",
    "        print(\"[Stack] Training Logistic OvR...\")\n",
    "        self.logi.fit(X, y)\n",
    "\n",
    "        print(\"[Stack] Training Meta Softmax...\")\n",
    "        p1 = self.knn.predict(X)\n",
    "        p2 = self.rf.predict(X)\n",
    "        p3 = self.logi.predict(X)\n",
    "\n",
    "        stacked_X = np.column_stack((p1, p2, p3))\n",
    "        self.meta.fit(stacked_X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        p1 = self.knn.predict(X)\n",
    "        p2 = self.rf.predict(X)\n",
    "        p3 = self.logi.predict(X)\n",
    "\n",
    "        stacked_X = np.column_stack((p1, p2, p3))\n",
    "        return self.meta.predict(stacked_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286404fa",
   "metadata": {},
   "source": [
    "stack_logistic_xgb_rf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "dc554d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Stacked_Logistic_XGB_RF:\n",
    "    \n",
    "\n",
    "    def __init__(self, logistic_model, xgb_model, rf_model, meta_knn):\n",
    "        self.logi = logistic_model\n",
    "        self.xgb = xgb_model\n",
    "        self.rf = rf_model\n",
    "        self.meta = meta_knn\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        print(\"\\n[Stack] Training Logistic OvR...\")\n",
    "        self.logi.fit(X, y)\n",
    "\n",
    "        print(\"[Stack] Training XGBoost...\")\n",
    "        self.xgb.fit(X, y)\n",
    "\n",
    "        print(\"[Stack] Training Random Forest...\")\n",
    "        self.rf.fit(X, y)\n",
    "\n",
    "        print(\"[Stack] Training Meta KNN...\")\n",
    "        p1 = self.logi.predict(X)\n",
    "        p2 = self.xgb.predict(X)\n",
    "        p3 = self.rf.predict(X)\n",
    "\n",
    "        stacked_X = np.column_stack((p1, p2, p3))\n",
    "\n",
    "        self.meta.fit(stacked_X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        p1 = self.logi.predict(X)\n",
    "        p2 = self.xgb.predict(X)\n",
    "        p3 = self.rf.predict(X)\n",
    "\n",
    "        stacked_X = np.column_stack((p1, p2, p3))\n",
    "        return self.meta.predict(stacked_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "ee489fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleStacker:\n",
    "    \n",
    "    def __init__(self, base_models, meta_model):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        print(\"\\n[STACK] Training base models...\")\n",
    "        base_preds = []\n",
    "\n",
    "        for model in self.base_models:\n",
    "            name = model.__class__.__name__\n",
    "            print(f\"[STACK] Training base model: {name}\")\n",
    "            model.fit(X, y)\n",
    "            base_preds.append(model.predict(X))\n",
    "\n",
    "        # Combine predictions as features\n",
    "        stacked_X = np.column_stack(base_preds)\n",
    "\n",
    "        print(\"[STACK] Training meta model...\")\n",
    "        self.meta_model.fit(stacked_X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        base_preds = []\n",
    "\n",
    "        for model in self.base_models:\n",
    "            base_preds.append(model.predict(X))\n",
    "\n",
    "        stacked_X = np.column_stack(base_preds)\n",
    "\n",
    "        return self.meta_model.predict(stacked_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c229e4c0",
   "metadata": {},
   "source": [
    "stack1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "d48539df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL: Stack: KNN + Softmax  Logistic\n",
      "================================================================================\n",
      "\n",
      "[STACK] Training base models...\n",
      "[STACK] Training base model: KNNClassifier\n",
      "[STACK] Training base model: SoftmaxRegression\n",
      "[Softmax] Epoch 5/25, Loss=1.9130\n",
      "[Softmax] Epoch 10/25, Loss=1.5746\n",
      "[Softmax] Epoch 15/25, Loss=1.3518\n",
      "[Softmax] Epoch 20/25, Loss=1.1987\n",
      "[Softmax] Epoch 25/25, Loss=1.0882\n",
      "[STACK] Training meta model...\n",
      "[OvR] Training class 0\n",
      "[OvR] Training class 1\n",
      "[OvR] Training class 2\n",
      "[OvR] Training class 3\n",
      "[OvR] Training class 4\n",
      "[OvR] Training class 5\n",
      "[OvR] Training class 6\n",
      "[OvR] Training class 7\n",
      "[OvR] Training class 8\n",
      "[OvR] Training class 9\n",
      "Accuracy        : 0.3922\n",
      "Macro Precision : 0.3505\n",
      "Macro Recall    : 0.3831\n",
      "Macro F1        : 0.3351\n",
      "Micro Precision : 0.3922\n",
      "Micro Recall    : 0.3922\n",
      "Micro F1        : 0.3922\n",
      "Training Time   : 1.89\n",
      "Prediction Time : 0.40\n"
     ]
    }
   ],
   "source": [
    "stack1 = SimpleStacker(\n",
    "    base_models = [\n",
    "        KNNClassifier(k=5),\n",
    "        SoftmaxRegression()\n",
    "    ],\n",
    "    meta_model = OvRLogisticRegression()\n",
    ")\n",
    "results.append(\n",
    "    evaluate_model(\"Stack: KNN + Softmax  Logistic\", stack1,\n",
    "                   X_train, y_train, X_val, y_val)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d1af43",
   "metadata": {},
   "source": [
    "stack 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "f9528cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack2 = SimpleStacker(\n",
    "    base_models=[\n",
    "        KNNClassifier(k=5),\n",
    "        RandomForestClassifier(...),\n",
    "        OvRLogisticRegression()\n",
    "    ],\n",
    "    meta_model=SoftmaxRegression()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "2dda534f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL: Stack: KNN + RF + Logistic  Softmax\n",
      "================================================================================\n",
      "\n",
      "[STACK] Training base models...\n",
      "[STACK] Training base model: KNNClassifier\n",
      "[STACK] Training base model: RandomForestClassifier\n",
      "[RandomForest] Trained 1/15 trees\n",
      "[RandomForest] Trained 5/15 trees\n",
      "[RandomForest] Trained 10/15 trees\n",
      "[RandomForest] Trained 15/15 trees\n",
      "[STACK] Training base model: OvRLogisticRegression\n",
      "[OvR] Training class 0\n",
      "[OvR] Training class 1\n",
      "[OvR] Training class 2\n",
      "[OvR] Training class 3\n",
      "[OvR] Training class 4\n",
      "[OvR] Training class 5\n",
      "[OvR] Training class 6\n",
      "[OvR] Training class 7\n",
      "[OvR] Training class 8\n",
      "[OvR] Training class 9\n",
      "[STACK] Training meta model...\n",
      "[Softmax] Epoch 5/25, Loss=2.1555\n",
      "[Softmax] Epoch 10/25, Loss=2.1349\n",
      "[Softmax] Epoch 15/25, Loss=2.1187\n",
      "[Softmax] Epoch 20/25, Loss=2.1039\n",
      "[Softmax] Epoch 25/25, Loss=2.0902\n",
      "Accuracy        : 0.1853\n",
      "Macro Precision : 0.1467\n",
      "Macro Recall    : 0.1873\n",
      "Macro F1        : 0.1357\n",
      "Micro Precision : 0.1853\n",
      "Micro Recall    : 0.1853\n",
      "Micro F1        : 0.1853\n",
      "Training Time   : 47.78\n",
      "Prediction Time : 0.55\n"
     ]
    }
   ],
   "source": [
    "stack2 = SimpleStacker(\n",
    "    base_models = [\n",
    "        KNNClassifier(k=5),\n",
    "        RandomForestClassifier(\n",
    "            n_estimators=15,\n",
    "            subsample=0.7,\n",
    "            max_depth=7,\n",
    "            min_samples_split=10,\n",
    "            feature_subsample=0.5,\n",
    "            n_thresholds=10,\n",
    "            num_classes=10\n",
    "        ),\n",
    "        OvRLogisticRegression()\n",
    "    ],\n",
    "    meta_model = SoftmaxRegression()\n",
    ")\n",
    "results.append(\n",
    "    evaluate_model(\"Stack: KNN + RF + Logistic  Softmax\", stack2,\n",
    "                   X_train, y_train, X_val, y_val)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b048304",
   "metadata": {},
   "source": [
    "stack 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "4ce7da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack3 = SimpleStacker(\n",
    "    base_models=[\n",
    "        OvRLogisticRegression(\n",
    "            lr=0.05,\n",
    "            epochs=8,\n",
    "            bs=128,\n",
    "            num_classes=10\n",
    "        ),\n",
    "\n",
    "        XGBoostMulti(\n",
    "            num_classes=10,\n",
    "            n_estimators=120,\n",
    "            lr=0.15\n",
    "        ),\n",
    "\n",
    "        RandomForestClassifier(\n",
    "            n_estimators=15,\n",
    "            subsample=0.7,\n",
    "            max_depth=7,\n",
    "            min_samples_split=10,\n",
    "            feature_subsample=0.5,\n",
    "            n_thresholds=10,\n",
    "            num_classes=10\n",
    "        )\n",
    "    ],\n",
    "    meta_model=KNNClassifier(k=5)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "62675889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL: Stack 3: Logistic + XGB + RF  KNN\n",
      "================================================================================\n",
      "\n",
      "[STACK] Training base models...\n",
      "[STACK] Training base model: OvRLogisticRegression\n",
      "[OvR] Training class 0\n",
      "[OvR] Training class 1\n",
      "[OvR] Training class 2\n",
      "[OvR] Training class 3\n",
      "[OvR] Training class 4\n",
      "[OvR] Training class 5\n",
      "[OvR] Training class 6\n",
      "[OvR] Training class 7\n",
      "[OvR] Training class 8\n",
      "[OvR] Training class 9\n",
      "[STACK] Training base model: XGBoostMulti\n",
      "\n",
      " TRAINING XGBOOST (MULTI-CLASS) \n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[STACK] Training base model: RandomForestClassifier\n",
      "[RandomForest] Trained 1/15 trees\n",
      "[RandomForest] Trained 5/15 trees\n",
      "[RandomForest] Trained 10/15 trees\n",
      "[RandomForest] Trained 15/15 trees\n",
      "[STACK] Training meta model...\n",
      "Accuracy        : 0.8675\n",
      "Macro Precision : 0.8674\n",
      "Macro Recall    : 0.8656\n",
      "Macro F1        : 0.8656\n",
      "Micro Precision : 0.8675\n",
      "Micro Recall    : 0.8675\n",
      "Micro F1        : 0.8675\n",
      "Training Time   : 141.33\n",
      "Prediction Time : 0.46\n"
     ]
    }
   ],
   "source": [
    "results.append(\n",
    "    evaluate_model(\"Stack 3: Logistic + XGB + RF  KNN\",\n",
    "                   stack3,\n",
    "                   X_train, y_train,\n",
    "                   X_val, y_val)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e518f7",
   "metadata": {},
   "source": [
    "knn+logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "99bea241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL: Stack: KNN + Logistic  Logistic\n",
      "================================================================================\n",
      "\n",
      "[STACK] Training base models...\n",
      "[STACK] Training base model: KNNClassifier\n",
      "[STACK] Training base model: OvRLogisticRegression\n",
      "[OvR] Training class 0\n",
      "[OvR] Training class 1\n",
      "[OvR] Training class 2\n",
      "[OvR] Training class 3\n",
      "[OvR] Training class 4\n",
      "[OvR] Training class 5\n",
      "[OvR] Training class 6\n",
      "[OvR] Training class 7\n",
      "[OvR] Training class 8\n",
      "[OvR] Training class 9\n",
      "[STACK] Training meta model...\n",
      "[OvR] Training class 0\n",
      "[OvR] Training class 1\n",
      "[OvR] Training class 2\n",
      "[OvR] Training class 3\n",
      "[OvR] Training class 4\n",
      "[OvR] Training class 5\n",
      "[OvR] Training class 6\n",
      "[OvR] Training class 7\n",
      "[OvR] Training class 8\n",
      "[OvR] Training class 9\n",
      "Accuracy        : 0.3986\n",
      "Macro Precision : 0.4612\n",
      "Macro Recall    : 0.3895\n",
      "Macro F1        : 0.3313\n",
      "Micro Precision : 0.3986\n",
      "Micro Recall    : 0.3986\n",
      "Micro F1        : 0.3986\n",
      "Training Time   : 2.42\n",
      "Prediction Time : 0.55\n"
     ]
    }
   ],
   "source": [
    "stack_knn_logistic = SimpleStacker(\n",
    "    base_models = [\n",
    "        KNNClassifier(k=5),\n",
    "        OvRLogisticRegression(lr=0.05, epochs=8, bs=128)\n",
    "    ],\n",
    "    meta_model = OvRLogisticRegression(lr=0.05, epochs=8, bs=128)\n",
    ")\n",
    "\n",
    "results.append(\n",
    "    evaluate_model(\n",
    "        \"Stack: KNN + Logistic  Logistic\",\n",
    "        stack_knn_logistic,\n",
    "        X_train, y_train, X_val, y_val\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1024889f",
   "metadata": {},
   "source": [
    "knn+xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "40510dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL: Stack: KNN + XGB  Logistic\n",
      "================================================================================\n",
      "\n",
      "[STACK] Training base models...\n",
      "[STACK] Training base model: KNNClassifier\n",
      "[STACK] Training base model: XGBoostMulti\n",
      "\n",
      " TRAINING XGBOOST (MULTI-CLASS) \n",
      "[XGB-Binary] Tree 20/90\n",
      "[XGB-Binary] Tree 40/90\n",
      "[XGB-Binary] Tree 60/90\n",
      "[XGB-Binary] Tree 80/90\n",
      "[XGB-Binary] Tree 20/90\n",
      "[XGB-Binary] Tree 40/90\n",
      "[XGB-Binary] Tree 60/90\n",
      "[XGB-Binary] Tree 80/90\n",
      "[XGB-Binary] Tree 20/90\n",
      "[XGB-Binary] Tree 40/90\n",
      "[XGB-Binary] Tree 60/90\n",
      "[XGB-Binary] Tree 80/90\n",
      "[XGB-Binary] Tree 20/90\n",
      "[XGB-Binary] Tree 40/90\n",
      "[XGB-Binary] Tree 60/90\n",
      "[XGB-Binary] Tree 80/90\n",
      "[XGB-Binary] Tree 20/90\n",
      "[XGB-Binary] Tree 40/90\n",
      "[XGB-Binary] Tree 60/90\n",
      "[XGB-Binary] Tree 80/90\n",
      "[XGB-Binary] Tree 20/90\n",
      "[XGB-Binary] Tree 40/90\n",
      "[XGB-Binary] Tree 60/90\n",
      "[XGB-Binary] Tree 80/90\n",
      "[XGB-Binary] Tree 20/90\n",
      "[XGB-Binary] Tree 40/90\n",
      "[XGB-Binary] Tree 60/90\n",
      "[XGB-Binary] Tree 80/90\n",
      "[XGB-Binary] Tree 20/90\n",
      "[XGB-Binary] Tree 40/90\n",
      "[XGB-Binary] Tree 60/90\n",
      "[XGB-Binary] Tree 80/90\n",
      "[XGB-Binary] Tree 20/90\n",
      "[XGB-Binary] Tree 40/90\n",
      "[XGB-Binary] Tree 60/90\n",
      "[XGB-Binary] Tree 80/90\n",
      "[XGB-Binary] Tree 20/90\n",
      "[XGB-Binary] Tree 40/90\n",
      "[XGB-Binary] Tree 60/90\n",
      "[XGB-Binary] Tree 80/90\n",
      "[STACK] Training meta model...\n",
      "[OvR] Training class 0\n",
      "[OvR] Training class 1\n",
      "[OvR] Training class 2\n",
      "[OvR] Training class 3\n",
      "[OvR] Training class 4\n",
      "[OvR] Training class 5\n",
      "[OvR] Training class 6\n",
      "[OvR] Training class 7\n",
      "[OvR] Training class 8\n",
      "[OvR] Training class 9\n",
      "Accuracy        : 0.4010\n",
      "Macro Precision : 0.3854\n",
      "Macro Recall    : 0.3896\n",
      "Macro F1        : 0.3331\n",
      "Micro Precision : 0.4010\n",
      "Micro Recall    : 0.4010\n",
      "Micro F1        : 0.4010\n",
      "Training Time   : 72.10\n",
      "Prediction Time : 0.54\n"
     ]
    }
   ],
   "source": [
    "stack_knn_xgb = SimpleStacker(\n",
    "    base_models = [\n",
    "        KNNClassifier(k=5),\n",
    "        XGBoostMulti(num_classes=10, n_estimators=90, lr=0.15)\n",
    "    ],\n",
    "    meta_model = OvRLogisticRegression(lr=0.05, epochs=8, bs=128)\n",
    ")\n",
    "\n",
    "results.append(\n",
    "    evaluate_model(\n",
    "        \"Stack: KNN + XGB  Logistic\",\n",
    "        stack_knn_xgb,\n",
    "        X_train, y_train, X_val, y_val\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "33a73158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL: STACK 98+: 5-Model Ensemble  XGB  Logistic\n",
      "================================================================================\n",
      "\n",
      "[STACK] Training base models...\n",
      "[STACK] Training base model: SoftmaxRegression\n",
      "[Softmax] Epoch 5/25, Loss=1.9130\n",
      "[Softmax] Epoch 10/25, Loss=1.5746\n",
      "[Softmax] Epoch 15/25, Loss=1.3518\n",
      "[Softmax] Epoch 20/25, Loss=1.1987\n",
      "[Softmax] Epoch 25/25, Loss=1.0882\n",
      "[STACK] Training base model: OvRLogisticRegression\n",
      "[OvR] Training class 0\n",
      "[OvR] Training class 1\n",
      "[OvR] Training class 2\n",
      "[OvR] Training class 3\n",
      "[OvR] Training class 4\n",
      "[OvR] Training class 5\n",
      "[OvR] Training class 6\n",
      "[OvR] Training class 7\n",
      "[OvR] Training class 8\n",
      "[OvR] Training class 9\n",
      "[STACK] Training base model: KNNClassifier\n",
      "[STACK] Training base model: XGBoostMulti\n",
      "\n",
      "=========== TRAINING XGBOOST (MULTI-CLASS) ===========\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[XGB-Binary] Tree 20/120\n",
      "[XGB-Binary] Tree 40/120\n",
      "[XGB-Binary] Tree 60/120\n",
      "[XGB-Binary] Tree 80/120\n",
      "[XGB-Binary] Tree 100/120\n",
      "[XGB-Binary] Tree 120/120\n",
      "[STACK] Training base model: RandomForestClassifier\n",
      "[RandomForest] Trained 1/15 trees\n",
      "[RandomForest] Trained 5/15 trees\n",
      "[RandomForest] Trained 10/15 trees\n",
      "[RandomForest] Trained 15/15 trees\n",
      "[STACK] Training meta model...\n",
      "\n",
      "[STACK] Training base models...\n",
      "[STACK] Training base model: XGBoostMulti\n",
      "\n",
      "=========== TRAINING XGBOOST (MULTI-CLASS) ===========\n",
      "[XGB-Binary] Tree 20/80\n",
      "[XGB-Binary] Tree 40/80\n",
      "[XGB-Binary] Tree 60/80\n",
      "[XGB-Binary] Tree 80/80\n",
      "[XGB-Binary] Tree 20/80\n",
      "[XGB-Binary] Tree 40/80\n",
      "[XGB-Binary] Tree 60/80\n",
      "[XGB-Binary] Tree 80/80\n",
      "[XGB-Binary] Tree 20/80\n",
      "[XGB-Binary] Tree 40/80\n",
      "[XGB-Binary] Tree 60/80\n",
      "[XGB-Binary] Tree 80/80\n",
      "[XGB-Binary] Tree 20/80\n",
      "[XGB-Binary] Tree 40/80\n",
      "[XGB-Binary] Tree 60/80\n",
      "[XGB-Binary] Tree 80/80\n",
      "[XGB-Binary] Tree 20/80\n",
      "[XGB-Binary] Tree 40/80\n",
      "[XGB-Binary] Tree 60/80\n",
      "[XGB-Binary] Tree 80/80\n",
      "[XGB-Binary] Tree 20/80\n",
      "[XGB-Binary] Tree 40/80\n",
      "[XGB-Binary] Tree 60/80\n",
      "[XGB-Binary] Tree 80/80\n",
      "[XGB-Binary] Tree 20/80\n",
      "[XGB-Binary] Tree 40/80\n",
      "[XGB-Binary] Tree 60/80\n",
      "[XGB-Binary] Tree 80/80\n",
      "[XGB-Binary] Tree 20/80\n",
      "[XGB-Binary] Tree 40/80\n",
      "[XGB-Binary] Tree 60/80\n",
      "[XGB-Binary] Tree 80/80\n",
      "[XGB-Binary] Tree 20/80\n",
      "[XGB-Binary] Tree 40/80\n",
      "[XGB-Binary] Tree 60/80\n",
      "[XGB-Binary] Tree 80/80\n",
      "[XGB-Binary] Tree 20/80\n",
      "[XGB-Binary] Tree 40/80\n",
      "[XGB-Binary] Tree 60/80\n",
      "[XGB-Binary] Tree 80/80\n",
      "[STACK] Training meta model...\n",
      "[OvR] Training class 0\n",
      "[OvR] Training class 1\n",
      "[OvR] Training class 2\n",
      "[OvR] Training class 3\n",
      "[OvR] Training class 4\n",
      "[OvR] Training class 5\n",
      "[OvR] Training class 6\n",
      "[OvR] Training class 7\n",
      "[OvR] Training class 8\n",
      "[OvR] Training class 9\n",
      "Accuracy        : 0.2097\n",
      "Macro Precision : 0.1093\n",
      "Macro Recall    : 0.1984\n",
      "Macro F1        : 0.1198\n",
      "Micro Precision : 0.2097\n",
      "Micro Recall    : 0.2097\n",
      "Micro F1        : 0.2097\n",
      "Training Time   : 170.07\n",
      "Prediction Time : 5.39\n"
     ]
    }
   ],
   "source": [
    "stack_5 = SimpleStacker(\n",
    "    base_models=[\n",
    "        SoftmaxRegression(lr=0.1, epochs=25),\n",
    "        OvRLogisticRegression(lr=0.05, epochs=8, bs=128),\n",
    "        KNNClassifier(k=5),\n",
    "        XGBoostMulti(num_classes=10, n_estimators=120, lr=0.15),\n",
    "        RandomForestClassifier(\n",
    "            n_estimators=15,\n",
    "            subsample=0.7,\n",
    "            max_depth=7,\n",
    "            min_samples_split=10,\n",
    "            feature_subsample=0.5,\n",
    "            n_thresholds=10,\n",
    "            num_classes=10\n",
    "        )\n",
    "    ],\n",
    "    meta_model=SimpleStacker(\n",
    "        base_models=[\n",
    "            XGBoostMulti(num_classes=10, n_estimators=80, lr=0.12)\n",
    "        ],\n",
    "        meta_model=OvRLogisticRegression(\n",
    "            lr=0.05, epochs=8, bs=128\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "results.append(\n",
    "    evaluate_model(\"STACK 98+: 5-Model Ensemble  XGB  Logistic\",\n",
    "                   stack_98,\n",
    "                   X_train, y_train,\n",
    "                   X_val, y_val)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
